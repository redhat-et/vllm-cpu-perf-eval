---
# ============================================================================
# vLLM Performance Testing - Main Inventory File
# ============================================================================
#
# QUICK START:
# 1. Replace the IP addresses below with your actual DUT and Load Generator IPs
# 2. Update ansible_user and ansible_ssh_private_key_file if different
# 3. Set HF_TOKEN environment variable: export HF_TOKEN=hf_xxxxx
# 4. Run tests!
#
# Everything else is pre-configured and ready to use.
# ============================================================================

# ============================================================================
# HOST CONFIGURATION - ⚠️ CHANGE THESE VALUES ⚠️
# ============================================================================

all:
  children:
    # DUT (Device Under Test) - Runs vLLM server
    dut:
      hosts:
        my-dut:
          # ⚠️ CHANGE: Your DUT's IP address or hostname
          ansible_host: host.example.com

          # ⚠️ CHANGE: SSH user (common: ec2-user, ubuntu, root)
          ansible_user: user

          # ⚠️ CHANGE: Path to SSH private key
          ansible_ssh_private_key_file: file.pem

      # DUT specific settings
      vars:
        # Optional: Mount directories for persistent caching
        # Set use_persistent_cache to false to disable volume mounts
        use_persistent_cache: false
        model_cache_dir: /var/lib/vllm-models
        log_dir: /tmp/vllm-logs

    # Load Generator - Runs benchmarking tools (GuideLLM, vllm bench serve)
    load_generator:
      hosts:
        my-loadgen:
          # ⚠️ CHANGE: Your Load Generator's IP address or hostname
          ansible_host: host2.example.com

          # ⚠️ CHANGE: SSH user
          ansible_user: user

          # ⚠️ CHANGE: Path to SSH private key
          ansible_ssh_private_key_file: file.pem

          # Benchmark configuration
          bench_config:
            # ⚠️ CHANGE: DUT's IP (use private IP if same VPC, otherwise public IP)
            vllm_host: host.example.com  # Should match DUT's ansible_host
            vllm_port: 8000

            # Results storage location on load generator
            # On macOS with Podman, use $HOME subdirectory for VM compatibility
            results_dir: "{{ lookup('env', 'HOME') }}/benchmark-results"

      # Load generator specific settings
      vars:
        log_dir: /tmp/vllm-logs

  # ============================================================================
  # GLOBAL CONFIGURATION - Pre-configured, no changes needed
  # ============================================================================

  vars:
    # SSH configuration
    ansible_ssh_common_args: "-o StrictHostKeyChecking=no"

    # Platform setup options
    platform_setup:
      numa_balancing_off: true        # Recommended for benchmarks
      thp_defrag_never: true          # Recommended for determinism
      preempt_full: false             # NOT recommended for ML workloads (DUT)
      # For load generator: set to true for better latency
      # Override per host group if needed

    # HuggingFace token configuration
    # Set: export HF_TOKEN=hf_xxxxx before running playbooks
    huggingface:
      token_source: "env"           # Options: env, file, vault, prompt
      token_env_var: "HF_TOKEN"

    # Health check configuration
    health_check:
      timeout: 300                  # Total timeout in seconds
      interval: 5                   # Check interval in seconds

    # Benchmark tool configuration
    benchmark_tool:
      guidellm:
        # Using locally built rootless container image
        container_image: "localhost/guidellm:latest"
        cpuset_cpus: "16-31"        # Load generator CPU allocation
        cpuset_mems: "0"
        profile: "sweep"            # Options: sweep, fixed, step

        # GuideLLM benchmark parameters (all optional, defaults shown)
        # These can be overridden via -e flag when running playbooks
        max_seconds: 200            # Maximum benchmark duration
        max_requests: 2000          # Maximum number of requests
        saturation_threshold: 0.98  # Saturation detection threshold (0-1)
        max_concurrency: 128        # Maximum concurrent requests
        cooldown: 30                # Cooldown period between tests (seconds)
        outputs: "html,json,csv"    # Output formats (comma-separated)
        exclude_throughput_target: false   # Exclude target throughput from results
        exclude_throughput_result: false   # Exclude result throughput from results

    # Container runtime settings
    container_runtime:
      engine: "podman"
      # ⚠️ CHANGE: Update to official vLLM image or your preferred version
      # This image includes CPU optimizations for performance testing
      image: "quay.io/mtahhan/vllm:0.14.0"
      security_opts:
        - "seccomp=unconfined"
      capabilities:
        - "SYS_NICE"
      network_mode: "host"
      shm_size: "4g"

    # vLLM server configuration
    vllm_container_name: "vllm-server"
    vllm_server:
      host: "0.0.0.0"
      port: 8000

    # vLLM Endpoint Mode Configuration
    # Supports testing against external vLLM deployments (cloud, k8s, production)
    vllm_endpoint:
      # Mode: "managed" (default) or "external"
      # - managed: Ansible starts/stops vLLM container on DUT (traditional behavior)
      # - external: Use existing vLLM endpoint (skip container management)
      mode: "managed"

      # External endpoint configuration (only used when mode=external)
      external:
        # Full URL to vLLM endpoint (required when mode=external)
        # Examples:
        #   - http://my-vllm-instance.example.com:8000
        #   - https://vllm-prod.company.com
        #   - http://10.0.1.50:8000 (direct IP)
        #   - http://vllm-lb.k8s.local:8000 (Kubernetes LoadBalancer)
        url: null

        # Optional API key authentication for secured endpoints
        api_key:
          enabled: false
          source: "env"              # Options: env, file, vault, prompt
          env_var: "VLLM_API_KEY"    # Used when source=env
          file_path: null            # Used when source=file (e.g., /etc/secrets/vllm-key)
          vault_var: null            # Used when source=vault (Ansible vault variable)

    # ============================================================================
    # TEST CONFIGURATIONS - Pre-configured workloads
    # ============================================================================

    test_configs:
      # Embedding model testing
      embedding:
        workload_type: "embedding"
        isl: 512
        osl: 1
        backend: "openai-embeddings"
        vllm_args:
          - "--dtype=bfloat16"
          - "--max-model-len=512"
        kv_cache_space: "1GiB"

      # LLM Generative models - Various workloads
      summarization:
        workload_type: "summarization"
        isl: 1024
        osl: 256
        backend: "openai-completions"
        vllm_args:
          - "--dtype=bfloat16"
          - "--no_enable_prefix_caching"
        kv_cache_space: "40GiB"

      chat:
        workload_type: "chat"
        isl: 512
        osl: 128
        backend: "openai-chat"
        vllm_args:
          - "--dtype=bfloat16"
          - "--no_enable_prefix_caching"
        kv_cache_space: "40GiB"

      code:
        workload_type: "code"
        isl: 2048
        osl: 512
        backend: "openai-completions"
        vllm_args:
          - "--dtype=bfloat16"
          - "--no_enable_prefix_caching"
        kv_cache_space: "60GiB"

      rag:
        workload_type: "rag"
        isl: 4096
        osl: 256
        backend: "openai-completions"
        vllm_args:
          - "--dtype=bfloat16"
          - "--no_enable_prefix_caching"
        kv_cache_space: "80GiB"

    # ============================================================================
    # CORE CONFIGURATIONS - Pre-configured CPU/NUMA settings
    # ============================================================================
    #
    # These are example configurations. The actual CPU ranges will be auto-detected
    # and allocated based on your hardware's NUMA topology when using auto-config.
    #
    # For manual configuration, adjust cpuset_cpus/cpuset_mems to match your hardware.
    # ============================================================================

    core_configs:
      # Small configurations (8-16 cores)
      - name: "8cores-single-socket"
        cores: 8
        cpuset_cpus: "0-7"          # Container CPU allocation
        cpuset_mems: "0"            # NUMA node
        tensor_parallel: 1
        # OMP settings OPTIONAL (omit for auto-configuration)

      - name: "16cores-single-socket"
        cores: 16
        cpuset_cpus: "0-15"
        cpuset_mems: "0"
        tensor_parallel: 1

      # Medium configurations (32 cores)
      - name: "32cores-single-socket"
        cores: 32
        cpuset_cpus: "0-31"
        cpuset_mems: "0"
        tensor_parallel: 1

      # Large configurations (64+ cores)
      - name: "64cores-single-socket-tp2"
        cores: 64
        cpuset_cpus: "0-63"
        cpuset_mems: "0"
        omp_num_threads: 32         # Manual OMP tuning for TP=2
        omp_threads_bind: "0-31|32-63"
        tensor_parallel: 2

      - name: "96cores-dual-socket-tp3"
        cores: 96
        cpuset_cpus: "0-95"
        cpuset_mems: "0,1"          # Multi-NUMA
        tensor_parallel: 3

      - name: "128cores-dual-socket-tp4"
        cores: 128
        cpuset_cpus: "0-127"
        cpuset_mems: "0,1"
        tensor_parallel: 4

# ============================================================================
# USAGE EXAMPLES
# ============================================================================
#
# 1. Test Ansible connectivity:
#    ansible -i inventory/hosts.yml all -m ping
#
# 2. Run platform setup (one-time, requires reboot):
#    ansible-playbook -i inventory/hosts.yml playbooks/common/setup-platform.yml
#    ansible -i inventory/hosts.yml all -b -m reboot
#
# 3. Run auto-configured test (simple):
#    export HF_TOKEN=hf_xxxxx
#    ansible-playbook -i inventory/hosts.yml \
#      playbooks/llm/run-guidellm-test-auto.yml \
#      -e "test_model=meta-llama/Llama-3.2-1B-Instruct" \
#      -e "workload_type=summarization" \
#      -e "requested_cores=16"
#
# 4. Run manual-configured test:
#    export HF_TOKEN=hf_xxxxx
#    ansible-playbook -i inventory/hosts.yml \
#      playbooks/llm/run-guidellm-test.yml \
#      -e "test_model=meta-llama/Llama-3.2-1B-Instruct" \
#      -e "workload_type=summarization" \
#      -e "core_config_name=16cores-single-socket"
#
# 5. Run core sweep (test multiple core counts):
#    export HF_TOKEN=hf_xxxxx
#    ansible-playbook -i inventory/hosts.yml \
#      playbooks/llm/run-core-sweep-auto.yml \
#      -e "test_model=meta-llama/Llama-3.2-1B-Instruct" \
#      -e "workload_type=summarization" \
#      -e "requested_cores_list=[8,16,32,64]"
#
# ============================================================================
