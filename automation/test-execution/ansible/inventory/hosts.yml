---
# ============================================================================
# vLLM Performance Testing - Main Inventory File
# ============================================================================
#
# QUICK START:
# 1. Replace the IP addresses below with your actual DUT and Load Generator IPs
# 2. Update ansible_user and ansible_ssh_private_key_file if different
# 3. Set HF_TOKEN environment variable: export HF_TOKEN=hf_xxxxx
# 4. Run tests!
#
# Everything else is pre-configured and ready to use.
# ============================================================================

# ============================================================================
# HOST CONFIGURATION - ⚠️ CHANGE THESE VALUES ⚠️
# ============================================================================

all:
  children:
    # DUT (Device Under Test) - Runs vLLM server
    dut:
      hosts:
        my-dut:
          # ⚠️ CHANGE: Your DUT's IP address or hostname
          ansible_host: ec2-3-16-130-204.us-east-2.compute.amazonaws.com

          # ⚠️ CHANGE: SSH user (common: ec2-user, ubuntu, root)
          ansible_user: ec2-user

          # ⚠️ CHANGE: Path to SSH private key
          ansible_ssh_private_key_file: ~/mtahhan.pem

      # DUT specific settings
      vars:
        # Optional: Mount directories for persistent caching
        # Set use_persistent_cache to false to disable volume mounts
        use_persistent_cache: false
        model_cache_dir: /var/lib/vllm-models
        log_dir: /tmp/vllm-logs

    # Load Generator - Runs benchmarking tools (GuideLLM, vllm bench serve)
    load_generator:
      hosts:
        my-loadgen:
          # ⚠️ CHANGE: Your Load Generator's IP address or hostname
          ansible_host: ec2-3-15-239-105.us-east-2.compute.amazonaws.com

          # ⚠️ CHANGE: SSH user
          ansible_user: ec2-user

          # ⚠️ CHANGE: Path to SSH private key
          ansible_ssh_private_key_file: ~/mtahhan.pem

          # Benchmark configuration
          bench_config:
            # ⚠️ CHANGE: DUT's IP (use private IP if same VPC, otherwise public IP)
            vllm_host: ec2-3-16-130-204.us-east-2.compute.amazonaws.com  # Should match DUT's ansible_host
            vllm_port: 8000

            # Results storage location on load generator
            results_dir: /tmp/benchmark-results

      # Load generator specific settings
      vars:
        log_dir: /tmp/vllm-logs

# ============================================================================
# GLOBAL CONFIGURATION - Pre-configured, no changes needed
# ============================================================================

  vars:
    # SSH configuration
    ansible_ssh_common_args: "-o StrictHostKeyChecking=no"

    # Platform setup options
    platform_setup:
      numa_balancing_off: true     # Recommended for benchmarks
      thp_defrag_never: true        # Recommended for determinism
      preempt_full: false           # NOT recommended for ML workloads

    # HuggingFace token configuration
    # Set: export HF_TOKEN=hf_xxxxx before running playbooks
    huggingface:
      token_source: "env"           # Options: env, file, vault, prompt
      token_env_var: "HF_TOKEN"

    # Health check configuration
    health_check:
      timeout: 300                  # Total timeout in seconds
      interval: 5                   # Check interval in seconds

    # Benchmark tool configuration
    benchmark_tool:
      guidellm:
        container_image: "quay.io/mtahhan/guidellm:saturation-fix"
        cpuset_cpus: "16-31"        # Load generator CPU allocation
        cpuset_mems: "0"
        profile: "sweep"            # Options: sweep, fixed, step
        max_seconds: 200            # Maximum benchmark duration
        max_requests: 2000          # Maximum number of requests
        saturation_threshold: 0.98  # Saturation detection threshold
        max_concurrency: 128        # Maximum concurrent requests
        cooldown: 30                # Cooldown period between tests
        outputs: "html,json,csv"    # Output formats (comma-separated)
        exclude_throughput_target: false   # Exclude target throughput from results
        exclude_throughput_result: false   # Exclude result throughput from results

    # Container runtime settings
    container_runtime:
      engine: "podman"
      image: "quay.io/mtahhan/vllm:0.14.0"
      security_opts:
        - "seccomp=unconfined"
      capabilities:
        - "SYS_NICE"
      network_mode: "host"
      shm_size: "4g"

    # vLLM server configuration
    vllm_container_name: "vllm-server"
    vllm_server:
      host: "0.0.0.0"
      port: 8000

# ============================================================================
# TEST CONFIGURATIONS - Pre-configured workloads
# ============================================================================

    test_configs:
      # Embedding model testing
      embedding:
        workload_type: "embedding"
        isl: 512
        osl: 1
        backend: "openai-embeddings"
        vllm_args:
          - "--dtype=bfloat16"
          - "--max-model-len=512"
        kv_cache_space: "1GiB"

      # LLM Generative models - Various workloads
      summarization:
        workload_type: "summarization"
        isl: 1024
        osl: 256
        backend: "openai-completions"
        vllm_args:
          - "--dtype=bfloat16"
          - "--no_enable_prefix_caching"
        kv_cache_space: "40GiB"

      chat:
        workload_type: "chat"
        isl: 512
        osl: 128
        backend: "openai-chat"
        vllm_args:
          - "--dtype=bfloat16"
          - "--no_enable_prefix_caching"
        kv_cache_space: "40GiB"

      code:
        workload_type: "code"
        isl: 2048
        osl: 512
        backend: "openai-completions"
        vllm_args:
          - "--dtype=bfloat16"
          - "--no_enable_prefix_caching"
        kv_cache_space: "60GiB"

      rag:
        workload_type: "rag"
        isl: 4096
        osl: 256
        backend: "openai-completions"
        vllm_args:
          - "--dtype=bfloat16"
          - "--no_enable_prefix_caching"
        kv_cache_space: "80GiB"

# ============================================================================
# CORE CONFIGURATIONS - Pre-configured CPU/NUMA settings
# ============================================================================
#
# These are example configurations. The actual CPU ranges will be auto-detected
# and allocated based on your hardware's NUMA topology when using auto-config.
#
# For manual configuration, adjust cpuset_cpus/cpuset_mems to match your hardware.
# ============================================================================

    core_configs:
      # Small configurations (8-16 cores)
      - name: "8cores-single-socket"
        cores: 8
        cpuset_cpus: "0-7"          # Container CPU allocation
        cpuset_mems: "0"            # NUMA node
        tensor_parallel: 1
        # OMP settings OPTIONAL (omit for auto-configuration)

      - name: "16cores-single-socket"
        cores: 16
        cpuset_cpus: "0-15"
        cpuset_mems: "0"
        tensor_parallel: 1

      # Medium configurations (32 cores)
      - name: "32cores-single-socket"
        cores: 32
        cpuset_cpus: "0-31"
        cpuset_mems: "0"
        tensor_parallel: 1

      # Large configurations (64+ cores)
      - name: "64cores-single-socket-tp2"
        cores: 64
        cpuset_cpus: "0-63"
        cpuset_mems: "0"
        omp_num_threads: 32         # Manual OMP tuning for TP=2
        omp_threads_bind: "0-31|32-63"
        tensor_parallel: 2

      - name: "96cores-dual-socket-tp3"
        cores: 96
        cpuset_cpus: "0-95"
        cpuset_mems: "0,1"          # Multi-NUMA
        tensor_parallel: 3

      - name: "128cores-dual-socket-tp4"
        cores: 128
        cpuset_cpus: "0-127"
        cpuset_mems: "0,1"
        tensor_parallel: 4

# ============================================================================
# USAGE EXAMPLES
# ============================================================================
#
# 1. Test Ansible connectivity:
#    ansible -i inventory/hosts.yml all -m ping
#
# 2. Run platform setup (one-time, requires reboot):
#    ansible-playbook -i inventory/hosts.yml playbooks/common/setup-platform.yml
#    ansible -i inventory/hosts.yml all -b -m reboot
#
# 3. Run auto-configured test (simple):
#    export HF_TOKEN=hf_xxxxx
#    ansible-playbook -i inventory/hosts.yml \
#      playbooks/llm/run-guidellm-test-auto.yml \
#      -e "test_model=meta-llama/Llama-3.2-1B-Instruct" \
#      -e "workload_type=summarization" \
#      -e "requested_cores=16"
#
# 4. Run manual-configured test:
#    export HF_TOKEN=hf_xxxxx
#    ansible-playbook -i inventory/hosts.yml \
#      playbooks/llm/run-guidellm-test.yml \
#      -e "test_model=meta-llama/Llama-3.2-1B-Instruct" \
#      -e "workload_type=summarization" \
#      -e "core_config_name=16cores-single-socket"
#
# 5. Run core sweep (test multiple core counts):
#    export HF_TOKEN=hf_xxxxx
#    ansible-playbook -i inventory/hosts.yml \
#      playbooks/llm/run-core-sweep-auto.yml \
#      -e "test_model=meta-llama/Llama-3.2-1B-Instruct" \
#      -e "workload_type=summarization" \
#      -e "requested_cores_list=[8,16,32,64]"
#
# ============================================================================
