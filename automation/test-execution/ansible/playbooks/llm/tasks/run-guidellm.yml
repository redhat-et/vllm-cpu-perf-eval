---
# Run GuideLLM Benchmark for LLM Generative Models
# Runs containerized GuideLLM on load generator

- name: Get workload configuration
  ansible.builtin.set_fact:
    workload_cfg: "{{ test_configs[workload_type] }}"
    guidellm_cfg: "{{ benchmark_tool.guidellm }}"
    core_cfg: "{{ core_configuration }}"

- name: Create results directory for this test
  ansible.builtin.file:
    path: "{{ bench_config.results_dir }}/{{ test_model | basename }}/{{ workload_type }}/{{ core_cfg.name }}"
    state: directory
    mode: "0777"
    recurse: true

- name: Ensure results directory has write permissions for container
  ansible.builtin.command:
    cmd: "chmod -R 777 {{ bench_config.results_dir }}/{{ test_model | basename }}/{{ workload_type }}/{{ core_cfg.name }}"
  changed_when: false

- name: Display GuideLLM configuration
  ansible.builtin.debug:
    msg:
      - "Running GuideLLM Benchmark"
      - "Target: http://{{ bench_config.vllm_host }}:{{ bench_config.vllm_port }}"
      - "Workload: {{ workload_type }} ({{ workload_cfg.isl }}:{{ workload_cfg.osl }})"
      - "Profile: {{ guidellm_cfg.profile }}"
      - "Max Requests: {{ guidellm_cfg.max_requests }}"
      - "Max Concurrency: {{ guidellm_cfg.max_concurrency }}"

- name: Pull GuideLLM container image
  containers.podman.podman_image:
    name: "{{ guidellm_cfg.container_image }}"
    state: present

- name: Prepare GuideLLM environment variables
  ansible.builtin.set_fact:
    guidellm_env:
      GUIDELLM_TARGET: "http://{{ bench_config.vllm_host }}:{{ bench_config.vllm_port }}"
      GUIDELLM_PROFILE: "{{ guidellm_cfg.profile }}"
      GUIDELLM_MAX_SECONDS: "{{ guidellm_cfg.max_seconds }}"
      GUIDELLM_MAX_REQUESTS: "{{ guidellm_cfg.max_requests }}"
      GUIDELLM__EXCLUDE_THROUGHPUT_TARGET: "{{ guidellm_cfg.exclude_throughput_target | string | lower }}"
      GUIDELLM__EXCLUDE_THROUGHPUT_RESULT: "{{ guidellm_cfg.exclude_throughput_result | string | lower }}"
      GUIDELLM__SATURATION_THRESHOLD: "{{ guidellm_cfg.saturation_threshold }}"
      GUIDELLM_DATA: "prompt_tokens={{ workload_cfg.isl }},output_tokens={{ workload_cfg.osl }}"
      GUIDELLM__MAX_CONCURRENCY: "{{ guidellm_cfg.max_concurrency }}"
      GUIDELLM_COOLDOWN: "{{ guidellm_cfg.cooldown }}"
      GUIDELLM_OUTPUTS: "{{ guidellm_cfg.outputs }}"
      HF_TOKEN: "{{ hf_token }}"

- name: Start GuideLLM benchmark container
  containers.podman.podman_container:
    name: "guidellm-{{ workload_type }}-{{ core_cfg.name }}"
    image: "{{ guidellm_cfg.container_image }}"
    state: started
    rm: false  # Don't auto-remove so we can check exit code
    detach: true  # Run in background so we can stream logs
    network: host
    cpuset_cpus: "{{ omit if ansible_system == 'Darwin' else guidellm_cfg.cpuset_cpus }}"
    cpuset_mems: "{{ omit if ansible_system == 'Darwin' else guidellm_cfg.cpuset_mems }}"
    volumes:
      - "{{ bench_config.results_dir }}/{{ test_model | basename }}/{{ workload_type }}/{{ core_cfg.name }}:/results:z"
    env: "{{ guidellm_env }}"
    log_driver: journald
    log_opt:
      tag: "guidellm-{{ workload_type }}-{{ core_cfg.name }}"
  register: guidellm_container

- name: Set monitoring command based on connection type
  ansible.builtin.set_fact:
    monitor_cmd: "{{ 'podman logs -f guidellm-' + workload_type + '-' + core_cfg.name if ansible_connection == 'local' else 'ssh ' + ansible_user + '@' + ansible_host + ' podman logs -f guidellm-' + workload_type + '-' + core_cfg.name }}"

- name: Display benchmark start info
  ansible.builtin.debug:
    msg:
      - "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
      - "GuideLLM Benchmark Started"
      - "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
      - "Container: {{ guidellm_container.container.Id[:12] }}"
      - "Max Duration: {{ guidellm_cfg.max_seconds }}s | Max Requests: {{ guidellm_cfg.max_requests }}"
      - "{{ 'Location: localhost' if ansible_connection == 'local' else 'Location: ' + ansible_host }}"
      - ""
      - "⚠️  Ansible will now wait silently for completion"
      - "⚠️  This may take several minutes (or hours for sweep tests)"
      - ""
      - "To monitor progress, open another terminal and run:"
      - "  {{ monitor_cmd }}"
      - "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

- name: Wait for GuideLLM benchmark to complete (this will block silently)
  ansible.builtin.command:
    cmd: "podman wait guidellm-{{ workload_type }}-{{ core_cfg.name }}"
  register: guidellm_exit_code
  changed_when: false
  failed_when: false

- name: Get container exit details
  ansible.builtin.command:
    cmd: "podman inspect guidellm-{{ workload_type }}-{{ core_cfg.name }} --format '{{{{.State.ExitCode}}}}'"
  register: container_exit_code
  changed_when: false
  failed_when: false

- name: Display GuideLLM completion status
  ansible.builtin.debug:
    msg:
      - "{{ '✓ GuideLLM benchmark completed successfully' if container_exit_code.stdout == '0' else '✗ GuideLLM benchmark failed' }}"
      - "Exit Code: {{ container_exit_code.stdout }}"
      - "Results: {{ bench_config.results_dir }}/{{ test_model | basename }}/{{ workload_type }}/{{ core_cfg.name }}"

- name: Remove completed container
  containers.podman.podman_container:
    name: "guidellm-{{ workload_type }}-{{ core_cfg.name }}"
    state: absent
  when: container_exit_code.stdout is defined

- name: Fail if GuideLLM encountered errors
  ansible.builtin.fail:
    msg: "GuideLLM benchmark failed with exit code {{ container_exit_code.stdout }}. Check logs above for details."
  when:
    - container_exit_code.stdout is defined
    - container_exit_code.stdout != '0'

- name: List generated result files
  ansible.builtin.find:
    paths: "{{ bench_config.results_dir }}/{{ test_model | basename }}/{{ workload_type }}/{{ core_cfg.name }}"
    patterns: "*.html,*.json,*.csv"
  register: result_files

- name: Display result files
  ansible.builtin.debug:
    msg: "Generated {{ result_files.files | length }} result files: {{ result_files.files | map(attribute='path') | map('basename') | list }}"
