---
# Baseline performance test tasks (sweep test)
# Included by run-embedding-tests.yml

- name: Set vllm-bench execution mode
  ansible.builtin.set_fact:
    use_vllm_bench_container: "{{ benchmark_tool.vllm_bench.use_container | default(true) }}"
    vllm_bench_image: "{{ benchmark_tool.vllm_bench.container_image | default('quay.io/mtahhan/vllm:arm-base-cpu') }}"
    num_prompts: "{{ benchmark_tool.vllm_bench.num_prompts | default(250) }}"

- name: "Baseline Test - Find Maximum Throughput (inf rate) - Containerized"
  ansible.builtin.command:
    cmd: >
      podman run --rm --entrypoint="" --network host
      -v {{ bench_config.results_dir }}:{{ bench_config.results_dir }}:z
      {{ vllm_bench_image }}
      /opt/venv/bin/vllm bench serve
      --host {{ bench_config.vllm_host }}
      --port {{ bench_config.vllm_port }}
      --backend openai-embeddings
      --model {{ model_name }}
      --dataset-name random
      --random-input-len 512
      --num-prompts {{ num_prompts }}
      --request-rate inf
      --endpoint /v1/embeddings
      --save-result
      --result-filename {{ bench_config.results_dir }}/{{ model_name | basename }}/baseline/sweep-inf.json
  register: baseline_inf_container
  changed_when: true
  when: use_vllm_bench_container | bool

- name: "Baseline Test - Find Maximum Throughput (inf rate) - Host"
  ansible.builtin.command:
    cmd: >
      vllm bench serve
      --host {{ bench_config.vllm_host }}
      --port {{ bench_config.vllm_port }}
      --backend openai-embeddings
      --model {{ model_name }}
      --dataset-name random
      --random-input-len 512
      --num-prompts {{ num_prompts }}
      --request-rate inf
      --endpoint /v1/embeddings
      --save-result
      --result-filename {{ bench_config.results_dir }}/{{ model_name | basename }}/baseline/sweep-inf.json
  register: baseline_inf_host
  changed_when: true
  when: not (use_vllm_bench_container | bool)

- name: Set baseline_inf output
  ansible.builtin.set_fact:
    baseline_inf_output: "{{ (baseline_inf_container.stdout | default('')) if use_vllm_bench_container else (baseline_inf_host.stdout | default('')) }}"

- name: Validate baseline test output
  ansible.builtin.assert:
    that:
      - baseline_inf_output is defined
      - baseline_inf_output | length > 0
      - baseline_inf_output is search('Request throughput \(req/s\)')
    fail_msg: |
      Failed to validate vllm bench output. Expected format with 'Request throughput (req/s)' metric.
      Output received: {{ baseline_inf_output | default('No output') }}
    success_msg: "Baseline test output validated"

- name: Parse maximum RPS from inf rate test
  ansible.builtin.set_fact:
    max_rps_match: "{{ baseline_inf_output | regex_search('Request throughput \\(req/s\\):\\s+([0-9.]+)', '\\1') }}"

- name: Validate RPS extraction
  ansible.builtin.assert:
    that:
      - max_rps_match is defined
      - max_rps_match | length > 0
      - max_rps_match[0] is defined
    fail_msg: |
      Failed to extract RPS value from output.
      Pattern: 'Request throughput (req/s): <number>'
      Output: {{ baseline_inf_output }}
    success_msg: "RPS value extracted successfully"

- name: Convert RPS to float
  ansible.builtin.set_fact:
    max_rps: "{{ max_rps_match[0] | float }}"

- name: Validate RPS value
  ansible.builtin.assert:
    that:
      - max_rps | float > 0
    fail_msg: "Invalid RPS value: {{ max_rps }}. Expected positive number."
    success_msg: "RPS value validated: {{ max_rps }}"

- name: Display maximum RPS
  ansible.builtin.debug:
    msg:
      - "Maximum RPS achieved: {{ max_rps }}"
      - "25% rate: {{ (max_rps * 0.25) | round(2) }}"
      - "50% rate: {{ (max_rps * 0.50) | round(2) }}"
      - "75% rate: {{ (max_rps * 0.75) | round(2) }}"

- name: "Baseline Test - 25% Load - Containerized"
  ansible.builtin.command:
    cmd: >
      podman run --rm --entrypoint="" --network host
      -v {{ bench_config.results_dir }}:{{ bench_config.results_dir }}:z
      {{ vllm_bench_image }}
      /opt/venv/bin/vllm bench serve
      --host {{ bench_config.vllm_host }}
      --port {{ bench_config.vllm_port }}
      --backend openai-embeddings
      --model {{ model_name }}
      --dataset-name random
      --random-input-len 512
      --num-prompts {{ num_prompts }}
      --request-rate {{ (max_rps * 0.25) | round(2) }}
      --endpoint /v1/embeddings
      --save-result
      --result-filename {{ bench_config.results_dir }}/{{ model_name | basename }}/baseline/sweep-25pct.json
  changed_when: true
  when: use_vllm_bench_container | bool

- name: "Baseline Test - 25% Load - Host"
  ansible.builtin.command:
    cmd: >
      vllm bench serve
      --host {{ bench_config.vllm_host }}
      --port {{ bench_config.vllm_port }}
      --backend openai-embeddings
      --model {{ model_name }}
      --dataset-name random
      --random-input-len 512
      --num-prompts {{ num_prompts }}
      --request-rate {{ (max_rps * 0.25) | round(2) }}
      --endpoint /v1/embeddings
      --save-result
      --result-filename {{ bench_config.results_dir }}/{{ model_name | basename }}/baseline/sweep-25pct.json
  changed_when: true
  when: not (use_vllm_bench_container | bool)

- name: "Baseline Test - 50% Load - Containerized"
  ansible.builtin.command:
    cmd: >
      podman run --rm --entrypoint="" --network host
      -v {{ bench_config.results_dir }}:{{ bench_config.results_dir }}:z
      {{ vllm_bench_image }}
      /opt/venv/bin/vllm bench serve
      --host {{ bench_config.vllm_host }}
      --port {{ bench_config.vllm_port }}
      --backend openai-embeddings
      --model {{ model_name }}
      --dataset-name random
      --random-input-len 512
      --num-prompts {{ num_prompts }}
      --request-rate {{ (max_rps * 0.50) | round(2) }}
      --endpoint /v1/embeddings
      --save-result
      --result-filename {{ bench_config.results_dir }}/{{ model_name | basename }}/baseline/sweep-50pct.json
  changed_when: true
  when: use_vllm_bench_container | bool

- name: "Baseline Test - 50% Load - Host"
  ansible.builtin.command:
    cmd: >
      vllm bench serve
      --host {{ bench_config.vllm_host }}
      --port {{ bench_config.vllm_port }}
      --backend openai-embeddings
      --model {{ model_name }}
      --dataset-name random
      --random-input-len 512
      --num-prompts {{ num_prompts }}
      --request-rate {{ (max_rps * 0.50) | round(2) }}
      --endpoint /v1/embeddings
      --save-result
      --result-filename {{ bench_config.results_dir }}/{{ model_name | basename }}/baseline/sweep-50pct.json
  changed_when: true
  when: not (use_vllm_bench_container | bool)

- name: "Baseline Test - 75% Load - Containerized"
  ansible.builtin.command:
    cmd: >
      podman run --rm --entrypoint="" --network host
      -v {{ bench_config.results_dir }}:{{ bench_config.results_dir }}:z
      {{ vllm_bench_image }}
      /opt/venv/bin/vllm bench serve
      --host {{ bench_config.vllm_host }}
      --port {{ bench_config.vllm_port }}
      --backend openai-embeddings
      --model {{ model_name }}
      --dataset-name random
      --random-input-len 512
      --num-prompts {{ num_prompts }}
      --request-rate {{ (max_rps * 0.75) | round(2) }}
      --endpoint /v1/embeddings
      --save-result
      --result-filename {{ bench_config.results_dir }}/{{ model_name | basename }}/baseline/sweep-75pct.json
  changed_when: true
  when: use_vllm_bench_container | bool

- name: "Baseline Test - 75% Load - Host"
  ansible.builtin.command:
    cmd: >
      vllm bench serve
      --host {{ bench_config.vllm_host }}
      --port {{ bench_config.vllm_port }}
      --backend openai-embeddings
      --model {{ model_name }}
      --dataset-name random
      --random-input-len 512
      --num-prompts {{ num_prompts }}
      --request-rate {{ (max_rps * 0.75) | round(2) }}
      --endpoint /v1/embeddings
      --save-result
      --result-filename {{ bench_config.results_dir }}/{{ model_name | basename }}/baseline/sweep-75pct.json
  changed_when: true
  when: not (use_vllm_bench_container | bool)

- name: Display baseline test completion
  ansible.builtin.debug:
    msg: "Baseline sweep tests completed. Results saved to {{ bench_config.results_dir }}/{{ model_name | basename }}/baseline/"
