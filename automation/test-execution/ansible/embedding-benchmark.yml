---
# Main playbook for running embedding model performance tests
# Two-node architecture: Start vLLM on DUT, wait for ready, then run tests from Load Generator

- name: "Embedding Performance Tests - Complete Workflow"
  hosts: localhost
  gather_facts: false
  vars:
    test_run_id: "{{ lookup('pipe', 'date +%Y%m%d-%H%M%S') }}"

  tasks:
    - name: Display test run information
      ansible.builtin.debug:
        msg:
          - "Test Run ID: {{ test_run_id }}"
          - "DUT Host: {{ groups['dut'][0] }}"
          - "Load Generator Host: {{ groups['load_generator'][0] }}"

# ==============================================================================
# PHASE 1: Prepare and Start vLLM Server on DUT
# ==============================================================================

- name: "Phase 1 - Start vLLM Server on DUT"
  hosts: dut
  become: true
  gather_facts: false
  vars:
    test_model: "{{ test_model | default('ibm-granite/granite-embedding-278m-multilingual') }}"
    workload_type: embedding

  roles:
    - role: vllm_server

# ==============================================================================
# PHASE 2: Health Check - Wait for vLLM to be Ready
# ==============================================================================

- name: "Phase 2 - Wait for vLLM Server to be Ready"
  hosts: load_generator
  gather_facts: false

  tasks:
    - name: Wait for vLLM health endpoint
      ansible.builtin.uri:
        url: "http://{{ bench_config.vllm_host }}:{{ bench_config.vllm_port }}/health"
        method: GET
        status_code: 200
      register: health_check_result
      retries: "{{ health_check.timeout // health_check.interval }}"
      delay: "{{ health_check.interval }}"
      until: health_check_result.status == 200

    - name: Display vLLM server status
      ansible.builtin.debug:
        msg:
          - "vLLM Server is READY!"
          - "Health check: {{ health_check_result.json | default('OK') }}"

    - name: Verify vLLM /v1/models endpoint
      ansible.builtin.uri:
        url: "http://{{ bench_config.vllm_host }}:{{ bench_config.vllm_port }}/v1/models"
        method: GET
        status_code: 200
      register: models_check

    - name: Display available models
      ansible.builtin.debug:
        msg: "Available models: {{ models_check.json.data | map(attribute='id') | list }}"

# ==============================================================================
# PHASE 3: Run Embedding Tests from Load Generator
# ==============================================================================

- name: "Phase 3 - Run Embedding Performance Tests"
  hosts: load_generator
  become: true
  vars:
    model_name: "{{ test_model | default('ibm-granite/granite-embedding-278m-multilingual') }}"
    test_type: "{{ scenario | default('baseline') }}"  # baseline, latency, or all

  pre_tasks:
    - name: Create results directory on load generator
      ansible.builtin.file:
        path: "{{ bench_config.results_dir }}/{{ model_name | replace('/', '__') }}/{{ test_type }}"
        state: directory
        mode: "0755"
        recurse: true

  roles:
    - role: benchmark_embedding

# ==============================================================================
# PHASE 4: Collect Logs and Results
# ==============================================================================

- name: "Phase 4 - Collect Logs and Results"
  ansible.builtin.import_playbook: collect-logs.yml

# ==============================================================================
# PHASE 5: Cleanup (Optional)
# ==============================================================================

- name: "Phase 5 - Cleanup (if requested)"
  hosts: dut
  become: true
  vars:
    cleanup: "{{ cleanup_after_test | default(false) }}"

  tasks:
    - name: Stop vLLM container
      containers.podman.podman_container:
        name: vllm-embedding-server
        state: stopped
      when:
        - cleanup | bool
        - container_runtime.engine == 'podman'

    - name: Display cleanup status
      ansible.builtin.debug:
        msg: "vLLM server {{ 'stopped' if cleanup else 'still running - use cleanup_after_test=true to stop' }}"
